{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb14375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dee22d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Paths to train and test folders\n",
    "# -------------------------------\n",
    "train_dir = r\"C:\\Users\\kotaa\\Downloads\\emotion_recognition_1\\emotion_recognition\\data\\train\"   # <-- change to your path\n",
    "test_dir  = r\"C:\\Users\\kotaa\\Downloads\\emotion_recognition_1\\emotion_recognition\\data\\test\"    # <-- change to your path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3676728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Image Data Generators\n",
    "# -------------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=64,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=64,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa4c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Build CNN Model\n",
    "# -------------------------------\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # auto detect number of emotions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf3d8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Compile Model\n",
    "# -------------------------------\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7784caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Callbacks\n",
    "# -------------------------------\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"cnn_emotion_model.h5\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "412b2211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.8116 - accuracy: 0.2488\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24868, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 153s 339ms/step - loss: 1.8116 - accuracy: 0.2488 - val_loss: 1.7906 - val_accuracy: 0.2487\n",
      "Epoch 2/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.7854 - accuracy: 0.2541\n",
      "Epoch 2: val_accuracy improved from 0.24868 to 0.29772, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 99s 221ms/step - loss: 1.7854 - accuracy: 0.2541 - val_loss: 1.7232 - val_accuracy: 0.2977\n",
      "Epoch 3/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.7498 - accuracy: 0.2750\n",
      "Epoch 3: val_accuracy improved from 0.29772 to 0.32182, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 126s 280ms/step - loss: 1.7498 - accuracy: 0.2750 - val_loss: 1.6743 - val_accuracy: 0.3218\n",
      "Epoch 4/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.7030 - accuracy: 0.3036\n",
      "Epoch 4: val_accuracy improved from 0.32182 to 0.39022, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 140s 312ms/step - loss: 1.7030 - accuracy: 0.3036 - val_loss: 1.5740 - val_accuracy: 0.3902\n",
      "Epoch 5/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.6266 - accuracy: 0.3501\n",
      "Epoch 5: val_accuracy improved from 0.39022 to 0.43508, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 143s 318ms/step - loss: 1.6266 - accuracy: 0.3501 - val_loss: 1.4588 - val_accuracy: 0.4351\n",
      "Epoch 6/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.5528 - accuracy: 0.3912\n",
      "Epoch 6: val_accuracy improved from 0.43508 to 0.47353, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 169s 376ms/step - loss: 1.5528 - accuracy: 0.3912 - val_loss: 1.3719 - val_accuracy: 0.4735\n",
      "Epoch 7/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.5053 - accuracy: 0.4145\n",
      "Epoch 7: val_accuracy improved from 0.47353 to 0.49498, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 166s 370ms/step - loss: 1.5053 - accuracy: 0.4145 - val_loss: 1.3154 - val_accuracy: 0.4950\n",
      "Epoch 8/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.4765 - accuracy: 0.4266\n",
      "Epoch 8: val_accuracy improved from 0.49498 to 0.50418, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 154s 342ms/step - loss: 1.4765 - accuracy: 0.4266 - val_loss: 1.2944 - val_accuracy: 0.5042\n",
      "Epoch 9/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.4351 - accuracy: 0.4446\n",
      "Epoch 9: val_accuracy improved from 0.50418 to 0.50822, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 153s 340ms/step - loss: 1.4351 - accuracy: 0.4446 - val_loss: 1.2808 - val_accuracy: 0.5082\n",
      "Epoch 10/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.4200 - accuracy: 0.4539\n",
      "Epoch 10: val_accuracy improved from 0.50822 to 0.53176, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 151s 336ms/step - loss: 1.4200 - accuracy: 0.4539 - val_loss: 1.2281 - val_accuracy: 0.5318\n",
      "Epoch 11/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.4027 - accuracy: 0.4575\n",
      "Epoch 11: val_accuracy improved from 0.53176 to 0.53385, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 140s 312ms/step - loss: 1.4027 - accuracy: 0.4575 - val_loss: 1.2180 - val_accuracy: 0.5339\n",
      "Epoch 12/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3792 - accuracy: 0.4688\n",
      "Epoch 12: val_accuracy did not improve from 0.53385\n",
      "449/449 [==============================] - 171s 379ms/step - loss: 1.3792 - accuracy: 0.4688 - val_loss: 1.2157 - val_accuracy: 0.5288\n",
      "Epoch 13/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3636 - accuracy: 0.4739\n",
      "Epoch 13: val_accuracy improved from 0.53385 to 0.55183, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 116s 257ms/step - loss: 1.3636 - accuracy: 0.4739 - val_loss: 1.1680 - val_accuracy: 0.5518\n",
      "Epoch 14/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3514 - accuracy: 0.4810\n",
      "Epoch 14: val_accuracy did not improve from 0.55183\n",
      "449/449 [==============================] - 113s 251ms/step - loss: 1.3514 - accuracy: 0.4810 - val_loss: 1.1710 - val_accuracy: 0.5469\n",
      "Epoch 15/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3383 - accuracy: 0.4858\n",
      "Epoch 15: val_accuracy improved from 0.55183 to 0.56227, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 112s 250ms/step - loss: 1.3383 - accuracy: 0.4858 - val_loss: 1.1454 - val_accuracy: 0.5623\n",
      "Epoch 16/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3261 - accuracy: 0.4919\n",
      "Epoch 16: val_accuracy improved from 0.56227 to 0.56283, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 111s 247ms/step - loss: 1.3261 - accuracy: 0.4919 - val_loss: 1.1456 - val_accuracy: 0.5628\n",
      "Epoch 17/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3272 - accuracy: 0.4911\n",
      "Epoch 17: val_accuracy did not improve from 0.56283\n",
      "449/449 [==============================] - 144s 322ms/step - loss: 1.3272 - accuracy: 0.4911 - val_loss: 1.1557 - val_accuracy: 0.5574\n",
      "Epoch 18/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3128 - accuracy: 0.4999\n",
      "Epoch 18: val_accuracy did not improve from 0.56283\n",
      "449/449 [==============================] - 116s 257ms/step - loss: 1.3128 - accuracy: 0.4999 - val_loss: 1.1540 - val_accuracy: 0.5581\n",
      "Epoch 19/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.3114 - accuracy: 0.4979\n",
      "Epoch 19: val_accuracy improved from 0.56283 to 0.56980, saving model to cnn_emotion_model.h5\n",
      "449/449 [==============================] - 102s 228ms/step - loss: 1.3114 - accuracy: 0.4979 - val_loss: 1.1332 - val_accuracy: 0.5698\n",
      "Epoch 20/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.2983 - accuracy: 0.5042\n",
      "Epoch 20: val_accuracy did not improve from 0.56980\n",
      "449/449 [==============================] - 102s 226ms/step - loss: 1.2983 - accuracy: 0.5042 - val_loss: 1.1216 - val_accuracy: 0.5677\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Train Model\n",
    "# -------------------------------\n",
    "epochs = 20\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[checkpoint, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6360d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 6s 54ms/step - loss: 1.1216 - accuracy: 0.5677\n",
      "âœ… Test Accuracy: 56.77%\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Evaluate Model\n",
    "# -------------------------------\n",
    "loss, acc = model.evaluate(validation_generator)\n",
    "print(f\"âœ… Test Accuracy: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d53721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Model training completed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Save final model\n",
    "model.save(\"cnn_emotion_model.h5\")\n",
    "print(\"ðŸŽ‰ Model training completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b99afdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"cnn_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
